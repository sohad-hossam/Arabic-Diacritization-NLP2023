{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sohad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>preprocessing and feature extraction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array1 = np.array([[1 ,0, 0],\n",
    "#  [1, 0, 0],\n",
    "#  [1 ,0, 0],\n",
    "#  [0, 0 ,1],\n",
    "#  [0 ,0, 1],\n",
    "#  [0 ,0 ,1]])\n",
    "\n",
    "# #stretched_arr = np.repeat(array1, repeats=3, axis=0)\n",
    "# array1=array1.reshape(2,3,3)\n",
    "# array2 = np.array([[[1,0,0], [1,1,0], [1,0,1]],\n",
    "#                    [[1,1,0], [1,0,0], [1,1,1]]])\n",
    "\n",
    "# print(array1)\n",
    "# result = np.append(array2,array1,axis=2)  # Append along rows\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "harakat_indeces_to_harakat  = {\n",
    "            0 : \"\",    # No Diacritic\n",
    "            1 : \"َ\",    # Fatha\n",
    "            2 : \"ً\" ,   # Fathatah\n",
    "            3 : \"ُ\" ,   # Damma\n",
    "            4 : \"ٌ\" ,   # Dammatan\n",
    "            5 : \"ِ\" ,   # Kasra\n",
    "            6 : \"ٍ\" ,   # Kasratan\n",
    "            7 : \"ْ\" ,   # Sukun\n",
    "            8 : \"ّ\" ,   # Shaddah\n",
    "            9 : \"َّ\" ,   # Shaddah + Fatha\n",
    "            10: \"ًّ\" ,   # Shaddah + Fathatah\n",
    "            11: \"ُّ\" ,   # Shaddah + Damma\n",
    "            12: \"ٌّ\" ,   # Shaddah + Dammatan\n",
    "            13: \"ِّ\" ,   # Shaddah + Kasra\n",
    "            14: \"ٍّ\" ,   # Shaddah + Kasratan\n",
    "                    \n",
    "        }\n",
    "\n",
    "arabic_alphabet_index_to_char = {\n",
    "                            1: \"ا\", \n",
    "                            2: \"ب\", \n",
    "                            3: \"ت\", \n",
    "                            4: \"ث\", \n",
    "                            5: \"ج\", \n",
    "                            6: \"ح\", \n",
    "                            7: \"خ\", \n",
    "                            8: \"د\", \n",
    "                            9: \"ذ\", \n",
    "                            10: \"ر\" ,\n",
    "                            11: \"ز\" ,\n",
    "                            12: \"س\" ,\n",
    "                            13: \"ش\" ,\n",
    "                            14: \"ص\" ,\n",
    "                            15: \"ض\" ,\n",
    "                            16: \"ط\" ,\n",
    "                            17: \"ظ\" ,\n",
    "                            18: \"ع\" ,\n",
    "                            19: \"غ\" ,\n",
    "                            20: \"ف\" ,\n",
    "                            21: \"ق\" ,\n",
    "                            22: \"ك\" ,\n",
    "                            23: \"ل\" ,\n",
    "                            24: \"م\" ,\n",
    "                            25: \"ن\" ,\n",
    "                            26: \"ه\" ,\n",
    "                            27: \"و\" ,\n",
    "                            28: \"ي\" ,\n",
    "                            29: \"آ\" ,\n",
    "                            30: \"إ\" ,\n",
    "                            31: \"ئ\" ,\n",
    "                            32: \"ء\" ,\n",
    "                            33: \"أ\" ,\n",
    "                            34:  \"ؤ\",\n",
    "                            35:  \"ة\",\n",
    "                            36:  \"ى\",\n",
    "                        }\n",
    "\n",
    "\n",
    "def Tokenizing(dataset_sentences):\n",
    "    harakat  = {\n",
    "                    \"\": 0,     # No Diacritic\n",
    "                    \"َ\": 1,     # Fatha\n",
    "                    \"ً\": 2,     # Fathatah\n",
    "                    \"ُ\": 3,     # Damma\n",
    "                    \"ٌ\": 4,     # Dammatan\n",
    "                    \"ِ\": 5,     # Kasra\n",
    "                    \"ٍ\": 6,     # Kasratan\n",
    "                    \"ْ\": 7,     # Sukun\n",
    "                    \"ّ\": 8,     # Shaddah\n",
    "                    \"َّ\": 9,     # Shaddah + Fatha\n",
    "                    \"ًّ\": 10,    # Shaddah + Fathatah\n",
    "                    \"ُّ\": 11,    # Shaddah + Damma\n",
    "                    \"ٌّ\": 12,    # Shaddah + Dammatan\n",
    "                    \"ِّ\": 13,    # Shaddah + Kasra\n",
    "                    \"ٍّ\": 14,    # Shaddah + Kasratan\n",
    "                            \n",
    "                }\n",
    "    \n",
    "    arabic_alphabet = {\n",
    "                            \"ا\": 1,\n",
    "                            \"ب\": 2,\n",
    "                            \"ت\": 3,\n",
    "                            \"ث\": 4,\n",
    "                            \"ج\": 5,\n",
    "                            \"ح\": 6,\n",
    "                            \"خ\": 7,\n",
    "                            \"د\": 8,\n",
    "                            \"ذ\": 9,\n",
    "                            \"ر\": 10,\n",
    "                            \"ز\": 11,\n",
    "                            \"س\": 12,\n",
    "                            \"ش\": 13,\n",
    "                            \"ص\": 14,\n",
    "                            \"ض\": 15,\n",
    "                            \"ط\": 16,\n",
    "                            \"ظ\": 17,\n",
    "                            \"ع\": 18,\n",
    "                            \"غ\": 19,\n",
    "                            \"ف\": 20,\n",
    "                            \"ق\": 21,\n",
    "                            \"ك\": 22,\n",
    "                            \"ل\": 23,\n",
    "                            \"م\": 24,\n",
    "                            \"ن\": 25,\n",
    "                            \"ه\": 26,\n",
    "                            \"و\": 27,\n",
    "                            \"ي\": 28,\n",
    "                            \"آ\": 29,\n",
    "                            \"إ\": 30,\n",
    "                            \"ئ\": 31,\n",
    "                            \"ء\": 32,\n",
    "                            \"أ\": 33,\n",
    "                            \"ؤ\":34,\n",
    "                            \"ة\":35,\n",
    "                            \"ى\":36,\n",
    "                        }\n",
    "    x_train_letters = []\n",
    "    y_train_letters = []\n",
    "    x_train_words = []\n",
    "    for sentence in dataset_sentences:\n",
    "        sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "        tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "        tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "        sentence_x = list()\n",
    "        sentence_y = list()\n",
    "        for word in tokens:\n",
    "            text, inputs, diacritics =util.extract_haraqat(word)\n",
    "            \n",
    "            word_x = list()\n",
    "            word_y = list()\n",
    "            for i in range(len(inputs)):\n",
    "                # for every char we see its rep in arabic_alphabet and get its 1 hot encoding vector\n",
    "                word_x.append(arabic_alphabet[inputs[i]])\n",
    "                word_y.append(harakat[diacritics[i]])\n",
    "                # word_x.append(inputs[i])\n",
    "                # word_y.append(diacritics[i])\n",
    "            sentence_x.append(word_x)\n",
    "            sentence_y.append(word_y)\n",
    "        if(len(sentence_x) != 0):\n",
    "            # 4d array len of sentences * len of words * len of chars * one hot encoding size\n",
    "            x_train_letters.append(sentence_x)\n",
    "            # 3d array len of sentences * len of words * number of diacritics --> represents the diacritics as indices check (harakat dict) \n",
    "            y_train_letters.append(sentence_y)\n",
    "\n",
    "        if(len(tokens_wihtout_diacratics)) :\n",
    "            x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "\n",
    "    print(y_train_letters[1])\n",
    "    print(len(x_train_letters), len(x_train_letters[1]), len(x_train_letters[1][1]), x_train_letters[1][1][1])\n",
    "    print(x_train_words[1])\n",
    "    return x_train_letters,y_train_letters,x_train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 7], [1, 1, 1], [0, 7, 1, 9, 3], [1, 1, 3], [0, 1, 7]]\n",
      "278416 5 3 16\n",
      "['أو', 'قطع', 'الأول', 'يده', 'إلخ']\n"
     ]
    }
   ],
   "source": [
    "dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "x_train_chars,y_train_chars,x_train_words = Tokenizing(dataset_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0], [3, 7, 1, 3], [5, 1, 0, 1, 3, 3]]\n",
      "14267 3 4 22\n",
      "['ولا', 'تكره', 'ضيافته']\n",
      "[[[21, 27, 23, 26]], [[27, 23, 1], [3, 22, 10, 26], [15, 28, 1, 20, 3, 26]], [[1, 23, 20, 10, 21], [1, 23, 4, 1, 23, 4], [27, 1, 23, 4, 23, 1, 4, 27, 25], [2, 28, 25], [21, 1, 18, 8, 35], [3, 21, 8, 24], [1, 23, 6, 22, 24], [18, 23, 36], [12, 2, 2, 26], [8, 27, 25], [13, 10, 16, 26], [33, 27], [13, 10, 16, 26], [8, 27, 25], [12, 2, 2, 26], [27, 2, 28, 25], [21, 1, 18, 8, 35], [3, 21, 8, 24, 26], [18, 23, 36], [1, 23, 12, 2, 2], [27, 1, 23, 13, 10, 16], [5, 24, 28, 18, 1]], [[27, 3, 6, 10, 28, 10, 26], [33, 25], [1, 23, 6, 22, 24], [30, 25], [22, 1, 25], [23, 26], [12, 2, 2], [2, 19, 28, 10], [13, 10, 16], [20, 3, 21, 8, 24], [18, 23, 28, 26], [23, 1], [28, 18, 3, 2, 10], [33, 27], [22, 1, 25], [23, 26], [12, 2, 2, 1, 25], [33, 27], [33, 12, 2, 1, 2], [20, 3, 21, 8, 24], [18, 23, 36], [5, 24, 28, 18, 26, 1], [23, 24], [28, 18, 3, 2, 10], [33, 27], [18, 23, 36], [2, 18, 15, 26, 1], [8, 27, 25], [2, 18, 15], [1, 18, 3, 2, 10], [2, 25, 1, 32], [18, 23, 36], [12, 2, 2], [1, 23, 7, 1, 14], [27, 23, 1], [28, 15, 10], [20, 21, 8, 1, 25], [2, 21, 28, 35], [1, 23, 33, 12, 2, 1, 2], [20, 30, 25], [13, 33, 25], [1, 23, 12, 2, 2], [33, 25], [28, 12, 3, 21, 23], [2, 4, 2, 27, 3], [24, 12, 2, 2, 26], [8, 27, 25], [19, 28, 10, 26], [24, 25], [1, 23, 33, 12, 2, 1, 2], [24, 4, 1, 23], [1, 23, 33, 27, 23], [1, 23, 11, 27, 1, 23], [12, 2, 2], [27, 5, 27, 2], [1, 23, 17, 26, 10], [20, 30, 9, 1], [14, 23, 28, 3], [21, 2, 23], [1, 23, 11, 27, 1, 23], [23, 24], [3, 18, 3, 2, 10], [17, 26, 10, 1], [27, 24, 4, 1, 23], [1, 23, 4, 1, 25, 28], [1, 23, 5, 23, 8], [23, 26], [4, 23, 1, 4, 35], [33, 12, 2, 1, 2], [1, 23, 11, 25, 36], [27, 1, 23, 21, 9, 20], [27, 1, 23, 13, 10, 2], [20, 24, 25], [5, 23, 8], [21, 2, 23], [24, 23, 1, 2, 12, 35], [13, 28, 32], [24, 25], [26, 9, 26], [1, 23, 4, 23, 1, 4, 35], [23, 24], [28, 18, 3, 2, 10], [9, 23, 22], [6, 8, 1], [27, 23, 1], [11, 5, 10, 1], [20, 26, 9, 1, 25], [21, 12, 24, 1, 25], [24, 1], [33, 18, 23, 24], [20, 28, 26, 24, 1], [7, 23, 1, 20, 1]], [[21, 27, 23, 26]], [[27, 26, 27]], [[33, 28]], [[1, 23, 2, 28, 18], [2, 1, 23, 24, 18, 25, 36], [1, 23, 4, 1, 25, 28], [1, 23, 9, 28], [26, 27], [1, 23, 18, 21, 8], [1, 23, 24, 9, 22, 27, 10], [26, 9, 1], [14, 10, 28, 6], [14, 25, 28, 18, 26], [27, 6, 28, 25, 31, 9], [28, 13, 22, 23], [21, 27, 23, 26]], [[24, 21, 1, 2, 23, 35], [13, 28, 32], [2, 13, 28, 32], [27, 21, 27, 23, 26]], [[24, 21, 1, 2, 23, 35], [24, 1, 23], [2, 24, 1, 23]]]\n",
      "[['قوله'], ['ولا', 'تكره', 'ضيافته'], ['الفرق', 'الثالث', 'والثلاثون', 'بين', 'قاعدة', 'تقدم', 'الحكم', 'على', 'سببه', 'دون', 'شرطه', 'أو', 'شرطه', 'دون', 'سببه', 'وبين', 'قاعدة', 'تقدمه', 'على', 'السبب', 'والشرط', 'جميعا'], ['وتحريره', 'أن', 'الحكم', 'إن', 'كان', 'له', 'سبب', 'بغير', 'شرط', 'فتقدم', 'عليه', 'لا', 'يعتبر', 'أو', 'كان', 'له', 'سببان', 'أو', 'أسباب', 'فتقدم', 'على', 'جميعها', 'لم', 'يعتبر', 'أو', 'على', 'بعضها', 'دون', 'بعض', 'اعتبر', 'بناء', 'على', 'سبب', 'الخاص', 'ولا', 'يضر', 'فقدان', 'بقية', 'الأسباب', 'فإن', 'شأن', 'السبب', 'أن', 'يستقل', 'بثبوت', 'مسببه', 'دون', 'غيره', 'من', 'الأسباب', 'مثال', 'الأول', 'الزوال', 'سبب', 'وجوب', 'الظهر', 'فإذا', 'صليت', 'قبل', 'الزوال', 'لم', 'تعتبر', 'ظهرا', 'ومثال', 'الثاني', 'الجلد', 'له', 'ثلاثة', 'أسباب', 'الزنى', 'والقذف', 'والشرب', 'فمن', 'جلد', 'قبل', 'ملابسة', 'شيء', 'من', 'هذه', 'الثلاثة', 'لم', 'يعتبر', 'ذلك', 'حدا', 'ولا', 'زجرا', 'فهذان', 'قسمان', 'ما', 'أعلم', 'فيهما', 'خلافا'], ['قوله'], ['وهو'], ['أي'], ['البيع', 'بالمعنى', 'الثاني', 'الذي', 'هو', 'العقد', 'المذكور', 'هذا', 'صريح', 'صنيعه', 'وحينئذ', 'يشكل', 'قوله'], ['مقابلة', 'شيء', 'بشيء', 'وقوله'], ['مقابلة', 'مال', 'بمال']]\n"
     ]
    }
   ],
   "source": [
    "dataset_txt = open(r\"val.txt\", \"r\", encoding='utf-8').read()\n",
    "chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "\n",
    "x_test_chars, y_test_chars, x_test_words = Tokenizing(dataset_sentences)\n",
    "print(x_test_chars[:10])\n",
    "print(x_test_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"my_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enconding = np.eye(37) \n",
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "\n",
    "word_index_x = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index_x.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "x_train_words_indices = [[word_index_x[word] for word in sentence] for sentence in x_train_words]\n",
    "\n",
    "x_test_words_indices=list()\n",
    "for sentence in x_test_words:\n",
    "    temp=list()\n",
    "    for word in sentence:\n",
    "        if(word_index_x.get(word)!=None):\n",
    "            temp.append(word_index_x[word])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    x_test_words_indices.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train_words, x_train_chars, y_train_chars, one_hot_enconding):\n",
    "        # the words dataset that will be given to the first model\n",
    "        max_list_length = max(len(_) for _ in x_train_words)\n",
    "        x_padded = [_ + [0] * (max_list_length - len(_)) for _ in x_train_words]\n",
    "        self.tensor_x_words = torch.tensor(x_padded)\n",
    "\n",
    "        x_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in x_train_chars]\n",
    "        #print(x_padded_chars[1])\n",
    "        #print(len(x_padded_chars[1]),len(x_padded_chars[1][1]))\n",
    "\n",
    "        # for sentence in x_train_chars:\n",
    "        #     temp=list()\n",
    "        #     for i in range(max_list_length - len(sentence)):\n",
    "        #         temp.append([])\n",
    "        #     sentence.extend(temp)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in x_train_chars for word in sentence)\n",
    "        for i in range(len(x_padded_chars)): #0->253\n",
    "            for j in range(len(x_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(x_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    x_padded_chars[i][j].extend(temp)\n",
    "        \n",
    "        #print(len(x_padded_chars), len(x_padded_chars[2]), len(x_padded_chars[2][1]), x_padded_chars[2][1][1])\n",
    "        self.tensor_x_chars = torch.tensor(x_padded_chars)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in y_train_chars for word in sentence)\n",
    "        #for sentence in y_train_chars: \n",
    "            #y_padded_chars.append([word + [0] * (max_char_length - len(word)) for word in sentence])\n",
    "        y_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in y_train_chars]    \n",
    "        for i in range(len(y_padded_chars)): #0->253\n",
    "            for j in range(len(y_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(y_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    y_padded_chars[i][j].extend(temp)\n",
    "        self.tensor_y_chars = torch.tensor(y_padded_chars)\n",
    "\n",
    "        #print(len(self.tensor_y_chars),len(self.tensor_y_chars[0]))\n",
    "        #print(self.tensor_x_words[0], self.tensor_x_chars[0], self.tensor_y_chars[0])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.tensor_x_chars)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_chars[idx], self.tensor_y_chars[idx], self.tensor_x_words[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Word2vec Feature Extraction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArabicDataset(x_train_words_indices, x_train_chars, y_train_chars, one_hot_enconding)\n",
    "\n",
    "#torch.save(dataset, 'ArabicDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ArabicDataset(x_test_words_indices, x_test_chars, y_test_chars, one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.tensor_x_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('./ArabicDataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> applying Bi-LSTM </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_Lstm(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, one_hot_enconding:np.array,hot_encoding_vector_len : int=37 , num_chars : int =37 , hidden_size_layer_words : int=32, hidden_size_layer_chars : int =32, classes : int=15):\n",
    "      super(Bi_Lstm, self).__init__()\n",
    "      self.embedding_words=nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n",
    "      self.lstm_layer_words = nn.LSTM(embedding_dim[1], hidden_size = hidden_size_layer_words, batch_first = True, bidirectional = True)\n",
    "      self.embedding_char=nn.Embedding(num_embeddings = num_chars, embedding_dim = hot_encoding_vector_len, _weight = torch.tensor(one_hot_enconding))\n",
    "      self.lstm_layer_char = nn.LSTM(hidden_size_layer_words * 2 + hot_encoding_vector_len, hidden_size=hidden_size_layer_chars, batch_first=True, bidirectional=True)\n",
    "      #self.lstm_layer_char = nn.LSTM(hot_encoding_vector_len, hidden_size = hidden_size_layer_chars, batch_first = True, bidirectional = True)\n",
    "\n",
    "      self.linear = nn.Linear(2 * hidden_size_layer_chars, classes, bias=True)\n",
    "\n",
    "  def forward(self,sentences,words):\n",
    "      feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "      feature_vector = feature_vector.to(dtype=torch.float)\n",
    "      lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "      #print(lstm_output[0].shape)  \n",
    "      # looping over lstm_output to remove unwanted feature vectors\n",
    "      no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "      lstm_output_words= lstm_output[0]\n",
    "      lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "      embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "      #words * chars * one hot encoding for each char\n",
    "      embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "      #print(embedding_char_output_3d.shape)\n",
    "\n",
    "      #print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "      embedding_char_output_3d_concatenated=np.zeros((embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1]))\n",
    "\n",
    "      # for word in range(len(lstm_output_words_2d)):\n",
    "      #        for chars in range(len(embedding_char_output_3d[word])):\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "      lstm_output_words_2d=lstm_output_words_2d.detach().numpy()\n",
    "\n",
    "      lstm_output_words_2d = np.repeat(lstm_output_words_2d, repeats=embedding_char_output_3d.shape[1], axis=0)\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "\n",
    "      lstm_output_words_2d=lstm_output_words_2d.reshape(lstm_output_words_2d.shape[0]//embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[1],lstm_output_words_2d.shape[1])\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "\n",
    "      embedding_char_output_3d_concatenated = np.append(embedding_char_output_3d.detach().numpy(),lstm_output_words_2d,axis=2)  # Append along rows\n",
    "\n",
    "      embedding_char_output_3d_concatenated = torch.tensor(embedding_char_output_3d_concatenated)\n",
    "      #print(embedding_char_output_3d_concatenated.shape)\n",
    "\n",
    "      embedding_char_output_3d_concatenated = embedding_char_output_3d_concatenated.to(dtype=torch.float)\n",
    "      lstm_output_char = self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "      final_output = self.linear(lstm_output_char[0])  # words*13 char * 15 classs\n",
    "\n",
    "      return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Bi_Lstm(embedding_matrix,(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size),one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(dataset.tensor_x_words[:100],dataset.tensor_x_chars[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=200, epochs=1, learning_rate=0.01):\n",
    "      ############################## TODO: replace the Nones in the following code ##################################\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples=0\n",
    "\n",
    "        for train_input_words, train_label, train_input_sentences in tqdm(train_dataloader):\n",
    "            # print(train_input_words.shape, train_label.shape, train_input_sentences.shape)\n",
    "            # (6) do the forward pass\n",
    "            output = model.forward(train_input_sentences, train_input_words)\n",
    "            \n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            # label: batch_size * 253 * 13\n",
    "            # output: 25300 word * 13 char * 15 \n",
    "            # k2eny fket el array l words msh sentence of words\n",
    "\n",
    "            batch_loss = criterion(output.view(-1, output.size(2)), train_label.view(-1))\n",
    "\n",
    "            # (8) append the batch loss to the total_loss_train\n",
    "\n",
    "            total_loss_train += batch_loss\n",
    "            \n",
    "            # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            _,predicted=torch.max(output,2)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n",
    "            acc=(predicted==train_label.view(-1, train_label.shape[2])).sum().item()\n",
    "            n_samples += train_label.size(0)*train_label.size(1)*train_label.size(2)\n",
    "            total_acc_train += acc\n",
    "            #print(100*total_acc_train/n_samples)\n",
    "\n",
    "            # (10) zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
    "\n",
    "            # (12) update the weights with your optimizer\n",
    "            optimizer.step()\n",
    "        # epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "        # (13) calculate the accuracy\n",
    "        epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataset)\n",
    "torch.save(model.state_dict(), 'Bi_lstm_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bi_Lstm(\n",
       "  (embedding_words): Embedding(105732, 100)\n",
       "  (lstm_layer_words): LSTM(100, 32, batch_first=True, bidirectional=True)\n",
       "  (embedding_char): Embedding(37, 37)\n",
       "  (lstm_layer_char): LSTM(101, 32, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=64, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Bi_lstm_Model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing second model for char level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letter = [dataset.harakat[diacritic] for diacritic in dataset.y_train_letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, outputFile,x_test_chars,batch_size=200):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  total_acc_test = 0\n",
    "  total_samples = 0\n",
    "  \n",
    "  total_acc_test_no_padding = 0\n",
    "  total_samples_no_padding = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    noBatch=0\n",
    "    for test_input_words, test_label, test_input_sentences in tqdm(test_dataloader):\n",
    "\n",
    "      output = model(test_input_sentences, test_input_words) # words * 13 char * 15 diacritic classes\n",
    "\n",
    "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "      _, predicted = torch.max(output, dim=2)  # words * 13 char \n",
    "\n",
    "      acc=(predicted==test_label.view(-1, test_label.shape[2])).sum().item()\n",
    "      total_acc_test += acc\n",
    "      total_samples += test_label.size(0)*test_label.size(1)*test_label.size(2)  # Update total samples count\n",
    "\n",
    "      #test_input_words=test_input_words.view(-1, test_input_words.shape[2]) #words*\n",
    "      predicted=predicted.view(test_label.shape[0],test_label.shape[1], test_label.shape[2])\n",
    "      temp = x_test_chars[noBatch*200:noBatch*200+200]\n",
    "\n",
    "      predicted=predicted.numpy()\n",
    "      for sentenceIndex in range(len(temp)): #iterate on every sentence\n",
    "        sentence=\"\"\n",
    "        for word in range(len(temp[sentenceIndex])):\n",
    "          temp_word_dicrated=\"\"\n",
    "          for char in range(len(temp[sentenceIndex][word])):\n",
    "            temp_word_dicrated+=arabic_alphabet_index_to_char[temp[sentenceIndex][word][char]]+harakat_indeces_to_harakat[predicted[sentenceIndex][word][char]]\n",
    "            if(predicted[sentenceIndex][word][char] == test_label[sentenceIndex][word][char]):\n",
    "              total_acc_test_no_padding+=1\n",
    "          total_samples_no_padding += len(temp[sentenceIndex][word])\n",
    "          temp_word_dicrated+=\" \"\n",
    "          sentence+=temp_word_dicrated\n",
    "        outputFile.append(sentence)\n",
    "      \n",
    "      noBatch+=1\n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test =100*total_acc_test/total_samples\n",
    "    total_acc_test_no_padding = 100*total_acc_test_no_padding/total_samples_no_padding\n",
    "  ##################################################################################################\n",
    "\n",
    "  print(outputFile[:10])\n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')\n",
    "  print(f'\\nTest Accuracy without padding: {total_acc_test_no_padding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [01:54<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ ', 'وَلَا تَكْرِهُ ضِيَافَتُهُ ', 'الْفَرْقِ الثَّالِثِ وَالثِّلَاثُونِ بَيْنَ قَاعِدَةِ تَقَدَّمَ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطُهُ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقِدَّمُهُ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا ', 'وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنَّ كَانَ لَهُ سَبَبُ بِغَيْرِ شَرْطِ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبْبَانٍ أَوْ أَسْبَابِ فَتَقَدَّمُ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرُ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضِ اعْتَبَرِ بِنَاءِ عَلَى سَبَبِ الْخَاصِ وَلَا يَضِرُّ فَقَدَانَ بِقِيَّةِ الْأَسْبَابِ فَإِنْ شَأْنَ السَّبَبِ أَنْ يَسْتَقِلَ بِثُبُوتِ مُسْبَبَهُ دُونَ غَيْرِهِ مِنْ الْأَسْبَابِ مَثَالِ الْأَوَّلِ الزَّوَالِ سَبَبُ وُجُوبِ الظَّهْرِ فَإِذَا صَلَيتُ قَبْلَ الزَّوَالِ لَمْ تَعْتَبَرِ ظُهُرًا وَمِثَالِ الثَّانِي الْجَلْدُ لَهُ ثَلَاثَةٍ أَسْبَابِ الزَّنَى وَالْقَذْفِ وَالشَّرْبِ فَمَنْ جَلْدٍ قَبْلَ مُلَابَسَةِ شَيْءٌ مِنْ هَذِهِ الثَّلَاثَةِ لَمْ يُعْتَبَرُ ذَلِكَ حَدًا وَلَا زَجْرًا فَهَذَانِ قِسْمَانِ مَا أَعْلَمُ فِيهِمَا خِلَافًا ', 'قَوْلُهُ ', 'وَهُوَ ', 'أَيْ ', 'الْبَيْعُ بِالْمَعْنَى الثَّانِي الَّذِي هُوَ الْعَقْدُ الْمَذْكُورِ هَذَا صَرِيحُ صَنِيعِهِ وَحِينَئِذِ يُشْكِلُ قَوْلُهُ ', 'مُقَابَلَةِ شَيْءٍ بِشَيْءٍ وَقَوْلُهُ ', 'مُقَابَلَةِ مَالٍ بِمَالِ ']\n",
      "\n",
      "Test Accuracy: 99.80806674925971\n",
      "\n",
      "Test Accuracy without padding: 85.98001895041308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputFile=list()\n",
    "evaluate(model, dataset_test, outputFile, x_test_chars)\n",
    "# print(x_test_chars[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
