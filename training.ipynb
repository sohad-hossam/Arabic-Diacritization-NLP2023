{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>preprocessing and feature extraction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 7], [1, 1, 1], [0, 7, 1, 9, 3], [1, 1, 3], [0, 1, 7]]\n",
      "278416 5 3 16\n",
      "['أو', 'قطع', 'الأول', 'يده', 'إلخ']\n"
     ]
    }
   ],
   "source": [
    "one_hot_enconding = np.eye(37) \n",
    "def Tokenizing(dataset_sentences):\n",
    "    harakat  = {\n",
    "                    \"\": 0,     # No Diacritic\n",
    "                    \"َ\": 1,     # Fatha\n",
    "                    \"ً\": 2,     # Fathatah\n",
    "                    \"ُ\": 3,     # Damma\n",
    "                    \"ٌ\": 4,     # Dammatan\n",
    "                    \"ِ\": 5,     # Kasra\n",
    "                    \"ٍ\": 6,     # Kasratan\n",
    "                    \"ْ\": 7,     # Sukun\n",
    "                    \"ّ\": 8,     # Shaddah\n",
    "                    \"َّ\": 9,     # Shaddah + Fatha\n",
    "                    \"ًّ\": 10,    # Shaddah + Fathatah\n",
    "                    \"ُّ\": 11,    # Shaddah + Damma\n",
    "                    \"ٌّ\": 12,    # Shaddah + Dammatan\n",
    "                    \"ِّ\": 13,    # Shaddah + Kasra\n",
    "                    \"ٍّ\": 14,    # Shaddah + Kasratan\n",
    "                            \n",
    "                }\n",
    "        \n",
    "    arabic_alphabet = {\n",
    "                            \"ا\": 1,\n",
    "                            \"ب\": 2,\n",
    "                            \"ت\": 3,\n",
    "                            \"ث\": 4,\n",
    "                            \"ج\": 5,\n",
    "                            \"ح\": 6,\n",
    "                            \"خ\": 7,\n",
    "                            \"د\": 8,\n",
    "                            \"ذ\": 9,\n",
    "                            \"ر\": 10,\n",
    "                            \"ز\": 11,\n",
    "                            \"س\": 12,\n",
    "                            \"ش\": 13,\n",
    "                            \"ص\": 14,\n",
    "                            \"ض\": 15,\n",
    "                            \"ط\": 16,\n",
    "                            \"ظ\": 17,\n",
    "                            \"ع\": 18,\n",
    "                            \"غ\": 19,\n",
    "                            \"ف\": 20,\n",
    "                            \"ق\": 21,\n",
    "                            \"ك\": 22,\n",
    "                            \"ل\": 23,\n",
    "                            \"م\": 24,\n",
    "                            \"ن\": 25,\n",
    "                            \"ه\": 26,\n",
    "                            \"و\": 27,\n",
    "                            \"ي\": 28,\n",
    "                            \"آ\": 29,\n",
    "                            \"إ\": 30,\n",
    "                            \"ئ\": 31,\n",
    "                            \"ء\": 32,\n",
    "                            \"أ\": 33,\n",
    "                            \"ؤ\":34,\n",
    "                            \"ة\":35,\n",
    "                            \"ى\":36,\n",
    "                        }\n",
    "\n",
    "    x_train_letters = []\n",
    "    y_train_letters = []\n",
    "    x_train_words = []\n",
    "    for sentence in dataset_sentences:\n",
    "        sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "        tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "        tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "        sentence_x = list()\n",
    "        sentence_y = list()\n",
    "        for word in tokens:\n",
    "            text, inputs, diacritics =util.extract_haraqat(word)\n",
    "            \n",
    "            word_x = list()\n",
    "            word_y = list()\n",
    "            for i in range(len(inputs)):\n",
    "                # for every char we see its rep in arabic_alphabet and get its 1 hot encoding vector\n",
    "                word_x.append(arabic_alphabet[inputs[i]])\n",
    "                word_y.append(harakat[diacritics[i]])\n",
    "                # word_x.append(inputs[i])\n",
    "                # word_y.append(diacritics[i])\n",
    "            sentence_x.append(word_x)\n",
    "            sentence_y.append(word_y)\n",
    "        if(len(sentence_x) != 0):\n",
    "            # 4d array len of sentences * len of words * len of chars * one hot encoding size\n",
    "            x_train_letters.append(sentence_x)\n",
    "            # 3d array len of sentences * len of words * number of diacritics --> represents the diacritics as indices check (harakat dict) \n",
    "            y_train_letters.append(sentence_y)\n",
    "\n",
    "        if(len(tokens_wihtout_diacratics)) :\n",
    "            x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "\n",
    "    print(y_train_letters[1])\n",
    "    print(len(x_train_letters), len(x_train_letters[1]), len(x_train_letters[1][1]), x_train_letters[1][1][1])\n",
    "    print(x_train_words[1])\n",
    "    return x_train_letters,y_train_letters,x_train_words\n",
    "\n",
    "dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "\n",
    "x_train_chars,y_train_chars,x_train_words = Tokenizing(dataset_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"my_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "\n",
    "word_index_x = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index_x.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "x_train_words_indices = [[word_index_x[word] for word in sentence] for sentence in x_train_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train_words, x_train_chars, y_train_chars, one_hot_enconding):\n",
    "        # the words dataset that will be given to the first model\n",
    "        max_list_length = max(len(_) for _ in x_train_words)\n",
    "        x_padded = [_ + [0] * (max_list_length - len(_)) for _ in x_train_words]\n",
    "        self.tensor_x_words = torch.tensor(x_padded)\n",
    "\n",
    "        x_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in x_train_chars]\n",
    "        #print(x_padded_chars[1])\n",
    "        #print(len(x_padded_chars[1]),len(x_padded_chars[1][1]))\n",
    "\n",
    "        # for sentence in x_train_chars:\n",
    "        #     temp=list()\n",
    "        #     for i in range(max_list_length - len(sentence)):\n",
    "        #         temp.append([])\n",
    "        #     sentence.extend(temp)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in x_train_chars for word in sentence)\n",
    "        print(len(x_padded_chars), len(x_padded_chars[1]), len(x_padded_chars[1][1]), x_padded_chars[1][1][1])\n",
    "        print(len(x_padded_chars), len(x_padded_chars[2]), len(x_padded_chars[2][1]), x_padded_chars[2][1][1])\n",
    "        for i in range(len(x_padded_chars)): #0->253\n",
    "            for j in range(len(x_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(x_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    x_padded_chars[i][j].extend(temp)\n",
    "        \n",
    "        print(len(x_padded_chars[0]))\n",
    "        print(len(x_padded_chars), len(x_padded_chars[2]), len(x_padded_chars[2][1]), x_padded_chars[2][1][1])\n",
    "        self.tensor_x_chars = torch.tensor(x_padded_chars)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in y_train_chars for word in sentence)\n",
    "        y_padded_chars = list()\n",
    "        #for sentence in y_train_chars: \n",
    "            #y_padded_chars.append([word + [0] * (max_char_length - len(word)) for word in sentence])\n",
    "            \n",
    "        for i in range(len(y_padded_chars)): #0->253\n",
    "            for j in range(len(y_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(y_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    y_padded_chars[i][j].extend(temp)\n",
    "        self.tensor_y_chars = torch.tensor(y_padded_chars)\n",
    "\n",
    "        print(len(self.tensor_x_words),len(self.tensor_x_words[0]))\n",
    "        print(self.tensor_x_words[0], self.tensor_x_chars[0], self.tensor_y_chars[0])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_letters), len(self.x_train_words)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train_letters[idx], self.y_train_letters_numbered[idx], self.x_train_words[idx], self.y_train_words_numbered[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Word2vec Feature Extraction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278416 253 13 16\n",
      "278416 253 13 23\n",
      "253\n",
      "278416 253 13 23\n",
      "278416 253\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ArabicDataset(x_train_words_indices, x_train_chars, y_train_chars, one_hot_enconding)\n",
      "Cell \u001b[1;32mIn[21], line 51\u001b[0m, in \u001b[0;36mArabicDataset.__init__\u001b[1;34m(self, x_train_words, x_train_chars, y_train_chars, one_hot_enconding)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_y_chars \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_padded_chars)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_x_words),\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_x_words[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_x_words[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_x_chars[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_y_chars[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "dataset = ArabicDataset(x_train_words_indices, x_train_chars, y_train_chars, one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.tensor_x_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> applying Bi-LSTM </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_Lstm(nn.Module):\n",
    "\n",
    "  def __init__(self, embedding_dim: tuple, x_train_padded_size: int, hidden_size_layer_words: int, hidden_size_layer_chars: int, classes: int=14):\n",
    "      \n",
    "      self.embedding=nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim=embedding_dim[1], _weight=[embedding_matrix], input_length=x_train_padded_size)\n",
    "      self.lstm_first_layer = nn.LSTM(input_size=(x_train_padded_size,embedding_dim[1]), hidden_size=(#noSentences,x_train_padded_size,embedding_dim[1]), num_layers=1, batch_first=True , bias=True, bidirectional=True)\n",
    "      self.lstm_sec_layer = nn.LSTM(input_size=, hidden_size=hidden_size_layer2, num_layers=1, batch_first=True , bias=True, bidirectional=True)\n",
    "      self.linear = nn.Softmax(hidden_size_layer2, classes, bias=True)\n",
    "\n",
    "  def forward(self,sentences,dataset):\n",
    "      feature_vector = self.embedding(sentences)\n",
    "      lstm_output = self.lstm_first_layer(feature_vector)\n",
    "\n",
    "      # looping over lstm_output to remove unwanted feature vectors\n",
    "      no_of_sentences, length_of_sentence, feature_vector_size = lstm_output.shape\n",
    "\n",
    "      lstm_output_without_padding = list()\n",
    "      for i in range(no_of_sentences):\n",
    "          #dy kda list of 1d dimension\n",
    "          lstm_output_without_padding.extend(lstm_output[i, 1:len(dataset.x_train_words[i])-1, :])\n",
    "\n",
    "\n",
    "      one_hot_enconding = np.eye(37)\n",
    "\n",
    "      x_train_letter_concatenated = []\n",
    "\n",
    "      for i in range(len(dataset.x_train_letters)):\n",
    "          concatenated_arrays = []\n",
    "          for letter in dataset.x_train_letters[i]:\n",
    "              concatenated_arrays.append(np.concatenate([one_hot_enconding[dataset.arabic_alphabet[letter]], lstm_output_without_padding[i]]))\n",
    "          x_train_letter_concatenated.extend(concatenated_arrays)\n",
    "\n",
    "      final_output=self.lstm_sec_layer(x_train_letter_concatenated)\n",
    "\n",
    "\n",
    "      #[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل']]\n",
    "      return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lstm_output[1][0:12])\n",
    "# print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing second model for char level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over lstm_output to remove unwanted feature vectors\n",
    "no_of_sentences, length_of_sentence, feature_vector_size = lstm_output.shape\n",
    "\n",
    "lstm_output_without_padding = list()\n",
    "for i in range(no_of_sentences):\n",
    "    lstm_output_without_padding.extend(lstm_output[i, 1:len(dataset.x_train_words[i])-1, :])\n",
    "\n",
    "# print(lstm_output_without_padding[0:6])\n",
    "\n",
    "\n",
    "# x_train_letter_concatinated = [[np.concatenate(one_hot_enconding[letter_index], lstm_output_without_padding[i]) for letter_index in x_train_letter_indices[i]] for i in range(len(dataset.x_train_letters))]\n",
    "\n",
    "x_train_letter_concatenated = []\n",
    "\n",
    "for i in range(len(dataset.x_train_letters)):\n",
    "    concatenated_arrays = []\n",
    "    for letter in dataset.x_train_letters[i]:\n",
    "        concatenated_arrays.append(np.concatenate([one_hot_enconding[dataset.arabic_alphabet[letter]], lstm_output_without_padding[i]]))\n",
    "    x_train_letter_concatenated.extend(concatenated_arrays)\n",
    "\n",
    "\n",
    "#[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letter = [dataset.harakat[diacritic] for diacritic in dataset.y_train_letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model_char = Sequential()\n",
    "first_bidirectional_layer = Bidirectional(LSTM(units=64, return_sequences=True))\n",
    "bi_lstm_model_char.add(Dense(255, activation='softmax'))\n",
    "\n",
    "bi_lstm_model_char.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bi_lstm_model_char.fit(np.array(x_train_letter_concatenated), np.array(y_train_letter), epochs=10, batch_size=32)\n",
    "score = bi_lstm_model_char.evaluate(np.array(x_train_letter_concatenated), np.array(y_train_letter))\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "bi_lstm_model_char.save(\"char-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(max_word_length,))\n",
    "word_embedding = Embedding(input_dim=num_words, output_dim=embedding_dim)(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
