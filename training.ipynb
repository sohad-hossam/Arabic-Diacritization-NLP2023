{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sohad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>preprocessing and feature extraction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "harakat_indeces_to_harakat  = {\n",
    "            0 : \"\" ,    # No Diacritic\n",
    "            1 : \"َ\" ,    # Fatha\n",
    "            2 : \"ً\" ,   # Fathatah\n",
    "            3 : \"ُ\" ,   # Damma\n",
    "            4 : \"ٌ\" ,   # Dammatan\n",
    "            5 : \"ِ\" ,   # Kasra\n",
    "            6 : \"ٍ\" ,   # Kasratan\n",
    "            7 : \"ْ\" ,   # Sukun\n",
    "            8 : \"ّ\" ,   # Shaddah\n",
    "            9 : \"َّ\" ,   # Shaddah + Fatha\n",
    "            10: \"ًّ\" ,   # Shaddah + Fathatah\n",
    "            11: \"ُّ\" ,   # Shaddah + Damma\n",
    "            12: \"ٌّ\" ,   # Shaddah + Dammatan\n",
    "            13: \"ِّ\" ,   # Shaddah + Kasra\n",
    "            14: \"ٍّ\" ,   # Shaddah + Kasratan\n",
    "                    \n",
    "}\n",
    "\n",
    "arabic_alphabet_index_to_char = {\n",
    "                            1:  \"ا\" , \n",
    "                            2:  \"ب\" , \n",
    "                            3:  \"ت\" , \n",
    "                            4:  \"ث\" , \n",
    "                            5:  \"ج\" , \n",
    "                            6:  \"ح\" , \n",
    "                            7:  \"خ\" , \n",
    "                            8:  \"د\" , \n",
    "                            9:  \"ذ\" , \n",
    "                            10: \"ر\" ,\n",
    "                            11: \"ز\" ,\n",
    "                            12: \"س\" ,\n",
    "                            13: \"ش\" ,\n",
    "                            14: \"ص\" ,\n",
    "                            15: \"ض\" ,\n",
    "                            16: \"ط\" ,\n",
    "                            17: \"ظ\" ,\n",
    "                            18: \"ع\" ,\n",
    "                            19: \"غ\" ,\n",
    "                            20: \"ف\" ,\n",
    "                            21: \"ق\" ,\n",
    "                            22: \"ك\" ,\n",
    "                            23: \"ل\" ,\n",
    "                            24: \"م\" ,\n",
    "                            25: \"ن\" ,\n",
    "                            26: \"ه\" ,\n",
    "                            27: \"و\" ,\n",
    "                            28: \"ي\" ,\n",
    "                            29: \"آ\" ,\n",
    "                            30: \"إ\" ,\n",
    "                            31: \"ئ\" ,\n",
    "                            32: \"ء\" ,\n",
    "                            33: \"أ\" ,\n",
    "                            34:  \"ؤ\",\n",
    "                            35:  \"ة\",\n",
    "                            36:  \"ى\",\n",
    "}\n",
    "\n",
    "harakat  = {\n",
    "                \"\": 0 ,     # No Diacritic\n",
    "                \"َ\": 1 ,     # Fatha\n",
    "                \"ً\": 2 ,     # Fathatah\n",
    "                \"ُ\": 3 ,     # Damma\n",
    "                \"ٌ\": 4 ,     # Dammatan\n",
    "                \"ِ\": 5 ,     # Kasra\n",
    "                \"ٍ\": 6 ,     # Kasratan\n",
    "                \"ْ\": 7 ,     # Sukun\n",
    "                \"ّ\": 8 ,     # Shaddah\n",
    "                \"َّ\": 9 ,     # Shaddah + Fatha\n",
    "                \"ًّ\": 10,     # Shaddah + Fathatah\n",
    "                \"ُّ\": 11,     # Shaddah + Damma\n",
    "                \"ٌّ\": 12,     # Shaddah + Dammatan\n",
    "                \"ِّ\": 13,     # Shaddah + Kasra\n",
    "                \"ٍّ\": 14,     # Shaddah + Kasratan\n",
    "}\n",
    "    \n",
    "arabic_alphabet = {\n",
    "                        \"ا\": 1 ,\n",
    "                        \"ب\": 2 ,\n",
    "                        \"ت\": 3 ,\n",
    "                        \"ث\": 4 ,\n",
    "                        \"ج\": 5 ,\n",
    "                        \"ح\": 6 ,\n",
    "                        \"خ\": 7 ,\n",
    "                        \"د\": 8 ,\n",
    "                        \"ذ\": 9 ,\n",
    "                        \"ر\": 10,\n",
    "                        \"ز\": 11,\n",
    "                        \"س\": 12,\n",
    "                        \"ش\": 13,\n",
    "                        \"ص\": 14,\n",
    "                        \"ض\": 15,\n",
    "                        \"ط\": 16,\n",
    "                        \"ظ\": 17,\n",
    "                        \"ع\": 18,\n",
    "                        \"غ\": 19,\n",
    "                        \"ف\": 20,\n",
    "                        \"ق\": 21,\n",
    "                        \"ك\": 22,\n",
    "                        \"ل\": 23,\n",
    "                        \"م\": 24,\n",
    "                        \"ن\": 25,\n",
    "                        \"ه\": 26,\n",
    "                        \"و\": 27,\n",
    "                        \"ي\": 28,\n",
    "                        \"آ\": 29,\n",
    "                        \"إ\": 30,\n",
    "                        \"ئ\": 31,\n",
    "                        \"ء\": 32,\n",
    "                        \"أ\": 33,\n",
    "                        \"ؤ\": 34,\n",
    "                        \"ة\": 35,\n",
    "                        \"ى\": 36,\n",
    "}\n",
    "\n",
    "def Tokenizing(dataset_sentences: list[str]):\n",
    "    x_train_letters = []\n",
    "    y_train_letters = []\n",
    "    x_train_words   = []\n",
    "    for sentence in dataset_sentences:\n",
    "        sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "        tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "        tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "        sentence_x = list()\n",
    "        sentence_y = list()\n",
    "        for word in tokens:\n",
    "            text, inputs, diacritics =util.extract_haraqat(word)\n",
    "            word_x = list()\n",
    "            word_y = list()\n",
    "            for i in range(len(inputs)):\n",
    "                # for every char we see its rep in arabic_alphabet and get its 1 hot encoding vector\n",
    "                word_x.append(arabic_alphabet[inputs[i]])\n",
    "                word_y.append(harakat[diacritics[i]])\n",
    "            sentence_x.append(word_x)\n",
    "            sentence_y.append(word_y)\n",
    "\n",
    "        if(len(sentence_x) != 0):\n",
    "            # 4d array len of sentences * len of words * len of chars * one hot encoding size\n",
    "            x_train_letters.append(sentence_x)\n",
    "            # 3d array len of sentences * len of words * number of diacritics --> represents the diacritics as indices check (harakat dict) \n",
    "            y_train_letters.append(sentence_y)\n",
    "\n",
    "        if(len(tokens_wihtout_diacratics)) :\n",
    "            x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "\n",
    "    print(y_train_letters[1])\n",
    "    print(len(x_train_letters), len(x_train_letters[1]), len(x_train_letters[1][1]), x_train_letters[1][1][1])\n",
    "    print(x_train_words[1])\n",
    "    return x_train_letters,y_train_letters,x_train_words\n",
    "\n",
    "def readDataset(file_path: str):\n",
    "    dataset_txt = open(file_path, \"r\", encoding='utf-8').read()\n",
    "    chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "    dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "    dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "    x_chars, y_chars, x_words = Tokenizing(dataset_sentences)\n",
    "    return x_chars, y_chars, x_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 7], [1, 1, 1], [0, 7, 1, 9, 3], [1, 1, 3], [0, 1, 7]]\n",
      "278416 5 3 16\n",
      "['أو', 'قطع', 'الأول', 'يده', 'إلخ']\n"
     ]
    }
   ],
   "source": [
    "x_train_chars,y_train_chars,x_train_words = readDataset(r\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0], [3, 7, 1, 3], [5, 1, 0, 1, 3, 3]]\n",
      "14267 3 4 22\n",
      "['ولا', 'تكره', 'ضيافته']\n"
     ]
    }
   ],
   "source": [
    "x_test_chars, y_test_chars, x_test_words = readDataset(r\"val.txt\")\n",
    "x_test_chars_needed = copy.deepcopy(x_test_chars)\n",
    "# print(x_test_chars[:10])\n",
    "# print(x_test_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=x_train_words, vector_size=200, window=5, min_count=1, workers=4)\n",
    "model.save(\"my_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enconding = np.eye(37) \n",
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "word_index_x = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "\n",
    "def getEmbeddingMatrix(word_index_x: dict[str, int], word2vec_model: np.ndarray):\n",
    "    # The first index is reserved for padding other wise the embedding matrix is filled as feature vector\n",
    "    embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "    for word, idx in word_index_x.items():\n",
    "        embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "def getWordIndex(x_test_words: np.ndarray) -> list:\n",
    "    x_test_words_indices = list()\n",
    "    for sentence in x_test_words:\n",
    "        temp = list()\n",
    "        for word in sentence:\n",
    "            if(word_index_x.get(word) != None):\n",
    "                temp.append(word_index_x[word])\n",
    "            else:\n",
    "                # In the case of unknown words they are acted as padding\n",
    "                temp.append(0)\n",
    "        x_test_words_indices.append(temp)\n",
    "    return x_test_words_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = getEmbeddingMatrix(word_index_x, word2vec_model)\n",
    "# getting the words as indicies\n",
    "x_train_words_indices = getWordIndex(x_train_words)\n",
    "x_test_words_indices = getWordIndex(x_test_words)\n",
    "print(x_test_words_indices[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>using fast text as feature extraction mechanism</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext_model = FastText(sentences=x_train_words, vector_size=100, window=5, min_count=1, workers=4, min_n=13, max_n=13)\n",
    "# fasttext_model.save(\"fast_text_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_enconding = np.eye(37) \n",
    "# FastText_model = FastText.load(\"fast_text_model\")\n",
    "# word_index_x = {word: idx + 1 for idx, word in enumerate(FastText_model.wv.index_to_key)}\n",
    "\n",
    "# def getEmbeddingMatrixFastText(word_index_x: dict[str, int], FastText_model: np.ndarray):\n",
    "#     # The first index is reserved for padding other wise the embedding matrix is filled as feature vector\n",
    "#     embedding_matrix = np.zeros((len(FastText_model.wv.index_to_key) + 1, FastText_model.vector_size))\n",
    "#     for word, idx in word_index_x.items():\n",
    "#         embedding_matrix[idx] = FastText_model.wv[word]\n",
    "#     return embedding_matrix\n",
    "\n",
    "# def getWordIndexFastText(x_test_words: np.ndarray) -> list:\n",
    "#     x_test_words_indices = list()\n",
    "#     for sentence in x_test_words:\n",
    "#         temp = list()\n",
    "#         for word in sentence:\n",
    "#             if(word_index_x.get(word) != None):\n",
    "#                 temp.append(word_index_x[word])\n",
    "#             else:\n",
    "#                 # In the case of unknown words they are acted as padding\n",
    "#                 temp.append(0)\n",
    "#         x_test_words_indices.append(temp)\n",
    "#     return x_test_words_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = getEmbeddingMatrixFastText(word_index_x, FastText_model)\n",
    "# print(x_train_words[0:10])\n",
    "# x_train_words_indices = getWordIndexFastText(x_train_words)\n",
    "# x_test_words_indices = getWordIndexFastText(x_test_words)\n",
    "# print(x_test_words_indices[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train_words, x_train_chars, y_train_chars):\n",
    "        # padding the x_word list\n",
    "        max_list_length = max(len(_) for _ in x_train_words)\n",
    "        x_padded = [_ + [0] * (max_list_length - len(_)) for _ in x_train_words]\n",
    "        # convert it to a tensor to be used in model\n",
    "        self.tensor_x_words = torch.tensor(x_padded)\n",
    "\n",
    "        # padding the sentences to desired sentence size\n",
    "        x_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in x_train_chars]\n",
    "        # getting the largest word\n",
    "        max_char_length = max(len(word) for sentence in x_train_chars for word in sentence)\n",
    "        for i in range(len(x_padded_chars)): # for every sentence\n",
    "            for j in range(len(x_padded_chars[i])):# for every word \n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(x_padded_chars[i][j])): # pad the words with the extra chars\n",
    "                    temp.append(0)\n",
    "                if(len(temp)!=0):\n",
    "                    x_padded_chars[i][j].extend(temp)\n",
    "        # convert it to a tensor to be used in model\n",
    "        self.tensor_x_chars = torch.tensor(x_padded_chars)\n",
    "\n",
    "        # padding the sentences to desired sentence size\n",
    "        max_char_length = max(len(word) for sentence in y_train_chars for word in sentence)\n",
    "\n",
    "        y_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in y_train_chars]    \n",
    "        for i in range(len(y_padded_chars)): # for every sentence\n",
    "            for j in range(len(y_padded_chars[i])): # for every word \n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(y_padded_chars[i][j])): # pad the words with the extra chars\n",
    "                    temp.append(0)\n",
    "                if(len(temp)!=0):\n",
    "                    y_padded_chars[i][j].extend(temp)\n",
    "        # convert labels to tensors to be used in model\n",
    "        self.tensor_y_chars = torch.tensor(y_padded_chars)\n",
    "        \n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_x_chars)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_chars[idx], self.tensor_y_chars[idx], self.tensor_x_words[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Getting the train and test dataset </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArabicDataset(x_train_words_indices, x_train_chars, y_train_chars)\n",
    "#torch.save(dataset, 'ArabicDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ArabicDataset(x_test_words_indices, x_test_chars, y_test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('./ArabicDataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> applying Bi-LSTM </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_Lstm(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, one_hot_enconding:np.array,hot_encoding_vector_len : int=37 , num_chars : int =37 , hidden_size_layer_words : int=32, hidden_size_layer_chars : int =32, classes : int=15):\n",
    "      super(Bi_Lstm, self).__init__()\n",
    "\n",
    "      # first embedding layer: input size --> vocab size, output size --> feature vector size(200), weigths --> word2vec, one hot encoding \n",
    "      self.embedding_words=nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n",
    "      # first bi-lstm layer(word level encoding): input size --> feature vector of every word, output size --> hidden layer size * 2\n",
    "      self.lstm_layer_words = nn.LSTM(embedding_dim[1], hidden_size = hidden_size_layer_words, batch_first = True, bidirectional = True)\n",
    "      # second embedding layer: input size --> vocab size, output size --> feature vector size(200), weigths --> word2vec, one hot encoding\n",
    "      self.embedding_char=nn.Embedding(num_embeddings = num_chars, embedding_dim = hot_encoding_vector_len, _weight = torch.tensor(one_hot_enconding))\n",
    "      self.lstm_layer_char = nn.LSTM(hidden_size_layer_words * 2 + hot_encoding_vector_len, hidden_size=hidden_size_layer_chars, batch_first=True, bidirectional=True)\n",
    "      #self.lstm_layer_char = nn.LSTM(hot_encoding_vector_len, hidden_size = hidden_size_layer_chars, batch_first = True, bidirectional = True)\n",
    "\n",
    "      self.linear = nn.Linear(2 * hidden_size_layer_chars, classes, bias=True)\n",
    "\n",
    "  def forward(self,sentences,words):\n",
    "      feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "      feature_vector = feature_vector.to(dtype=torch.float)\n",
    "      lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "      #print(lstm_output[0].shape)  \n",
    "      # looping over lstm_output to remove unwanted feature vectors\n",
    "      no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "      lstm_output_words= lstm_output[0]\n",
    "      lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "      embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "      #words * chars * one hot encoding for each char\n",
    "      embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "      #print(embedding_char_output_3d.shape)\n",
    "\n",
    "      #print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "      embedding_char_output_3d_concatenated=np.zeros((embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1]))\n",
    "\n",
    "      # for word in range(len(lstm_output_words_2d)):\n",
    "      #        for chars in range(len(embedding_char_output_3d[word])):\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "      lstm_output_words_2d=lstm_output_words_2d.detach().numpy()\n",
    "\n",
    "      lstm_output_words_2d = np.repeat(lstm_output_words_2d, repeats=embedding_char_output_3d.shape[1], axis=0)\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "\n",
    "      lstm_output_words_2d=lstm_output_words_2d.reshape(lstm_output_words_2d.shape[0]//embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[1],lstm_output_words_2d.shape[1])\n",
    "      #print(lstm_output_words_2d.shape)\n",
    "\n",
    "      embedding_char_output_3d_concatenated = np.append(embedding_char_output_3d.detach().numpy(),lstm_output_words_2d,axis=2)  # Append along rows\n",
    "\n",
    "      embedding_char_output_3d_concatenated = torch.tensor(embedding_char_output_3d_concatenated)\n",
    "      #print(embedding_char_output_3d_concatenated.shape)\n",
    "\n",
    "      embedding_char_output_3d_concatenated = embedding_char_output_3d_concatenated.to(dtype=torch.float)\n",
    "      lstm_output_char = self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "      final_output = self.linear(lstm_output_char[0])  # words * 13 char * 15 classs\n",
    "\n",
    "      return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Bi_Lstm(embedding_matrix,(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size),one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(dataset.tensor_x_words[:100],dataset.tensor_x_chars[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=200, epochs=1, learning_rate=0.01):\n",
    "      ############################## TODO: replace the Nones in the following code ##################################\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples=0\n",
    "\n",
    "        for train_input_words, train_label, train_input_sentences in tqdm(train_dataloader):\n",
    "            # print(train_input_words.shape, train_label.shape, train_input_sentences.shape)\n",
    "            # (6) do the forward pass\n",
    "            output = model.forward(train_input_sentences, train_input_words)\n",
    "            \n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            # label: batch_size * 253 * 13\n",
    "            # output: 25300 word * 13 char * 15 \n",
    "            # k2eny fket el array l words msh sentence of words\n",
    "\n",
    "            batch_loss = criterion(output.view(-1, output.size(2)), train_label.view(-1))\n",
    "\n",
    "            # (8) append the batch loss to the total_loss_train\n",
    "\n",
    "            total_loss_train += batch_loss\n",
    "            \n",
    "            # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            _,predicted=torch.max(output,2)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n",
    "            acc=(predicted==train_label.view(-1, train_label.shape[2])).sum().item()\n",
    "            n_samples += train_label.size(0)*train_label.size(1)*train_label.size(2)\n",
    "            total_acc_train += acc\n",
    "            #print(100*total_acc_train/n_samples)\n",
    "\n",
    "            # (10) zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
    "\n",
    "            # (12) update the weights with your optimizer\n",
    "            optimizer.step()\n",
    "        # epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "        # (13) calculate the accuracy\n",
    "        epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataset)\n",
    "torch.save(model.state_dict(), 'Bi_lstm_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Bi_lstm_Model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing second model for char level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letter = [dataset.harakat[diacritic] for diacritic in dataset.y_train_letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, outputFile,x_test_chars,batch_size=200):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  total_acc_test = 0\n",
    "  total_samples = 0\n",
    "  \n",
    "  total_acc_test_no_padding = 0\n",
    "  total_samples_no_padding = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    noBatch=0\n",
    "    for test_input_words, test_label, test_input_sentences in tqdm(test_dataloader):\n",
    "\n",
    "      output = model(test_input_sentences, test_input_words) # words * 13 char * 15 diacritic classes\n",
    "\n",
    "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "      _, predicted = torch.max(output, dim=2)  # words * 13 char \n",
    "\n",
    "      acc=(predicted==test_label.view(-1, test_label.shape[2])).sum().item()\n",
    "      total_acc_test += acc\n",
    "      total_samples += test_label.size(0)*test_label.size(1)*test_label.size(2)  # Update total samples count\n",
    "\n",
    "      #test_input_words=test_input_words.view(-1, test_input_words.shape[2]) #words*\n",
    "      predicted=predicted.view(test_label.shape[0],test_label.shape[1], test_label.shape[2])\n",
    "      temp = x_test_chars[noBatch*200:noBatch*200+200]\n",
    "\n",
    "      predicted=predicted.numpy()\n",
    "      for sentenceIndex in range(len(temp)): #iterate on every sentence\n",
    "        sentence=\"\"\n",
    "        for word in range(len(temp[sentenceIndex])):\n",
    "          temp_word_dicrated=\"\"\n",
    "          for char in range(len(temp[sentenceIndex][word])):\n",
    "            temp_word_dicrated+=arabic_alphabet_index_to_char[temp[sentenceIndex][word][char]]+harakat_indeces_to_harakat[predicted[sentenceIndex][word][char]]\n",
    "            if(predicted[sentenceIndex][word][char] == test_label[sentenceIndex][word][char]):\n",
    "              total_acc_test_no_padding+=1\n",
    "          total_samples_no_padding += len(temp[sentenceIndex][word])\n",
    "          temp_word_dicrated+=\" \"\n",
    "          sentence+=temp_word_dicrated\n",
    "        outputFile.append(sentence)\n",
    "      noBatch+=1\n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test =100*total_acc_test/total_samples\n",
    "    total_acc_test_no_padding = 100*total_acc_test_no_padding/total_samples_no_padding\n",
    "  ##################################################################################################\n",
    "\n",
    "  print(outputFile[:10])\n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')\n",
    "  print(f'\\nTest Accuracy without padding: {total_acc_test_no_padding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFile=list()\n",
    "evaluate(model, dataset_test, outputFile, x_test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'زركشي'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_index_x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mزركشي\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'زركشي'"
     ]
    }
   ],
   "source": [
    "# print(word_index_x['زركشي'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
