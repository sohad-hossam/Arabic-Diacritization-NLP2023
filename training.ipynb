{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\basse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>preprocessing and feature extraction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 7], [1, 1, 1], [0, 7, 1, 9, 3], [1, 1, 3], [0, 1, 7]]\n",
      "278416 5 3 16\n",
      "['أو', 'قطع', 'الأول', 'يده', 'إلخ']\n"
     ]
    }
   ],
   "source": [
    "one_hot_enconding = np.eye(37) \n",
    "def Tokenizing(dataset_sentences):\n",
    "    harakat  = {\n",
    "                    \"\": 0,     # No Diacritic\n",
    "                    \"َ\": 1,     # Fatha\n",
    "                    \"ً\": 2,     # Fathatah\n",
    "                    \"ُ\": 3,     # Damma\n",
    "                    \"ٌ\": 4,     # Dammatan\n",
    "                    \"ِ\": 5,     # Kasra\n",
    "                    \"ٍ\": 6,     # Kasratan\n",
    "                    \"ْ\": 7,     # Sukun\n",
    "                    \"ّ\": 8,     # Shaddah\n",
    "                    \"َّ\": 9,     # Shaddah + Fatha\n",
    "                    \"ًّ\": 10,    # Shaddah + Fathatah\n",
    "                    \"ُّ\": 11,    # Shaddah + Damma\n",
    "                    \"ٌّ\": 12,    # Shaddah + Dammatan\n",
    "                    \"ِّ\": 13,    # Shaddah + Kasra\n",
    "                    \"ٍّ\": 14,    # Shaddah + Kasratan\n",
    "                            \n",
    "                }\n",
    "        \n",
    "    arabic_alphabet = {\n",
    "                            \"ا\": 1,\n",
    "                            \"ب\": 2,\n",
    "                            \"ت\": 3,\n",
    "                            \"ث\": 4,\n",
    "                            \"ج\": 5,\n",
    "                            \"ح\": 6,\n",
    "                            \"خ\": 7,\n",
    "                            \"د\": 8,\n",
    "                            \"ذ\": 9,\n",
    "                            \"ر\": 10,\n",
    "                            \"ز\": 11,\n",
    "                            \"س\": 12,\n",
    "                            \"ش\": 13,\n",
    "                            \"ص\": 14,\n",
    "                            \"ض\": 15,\n",
    "                            \"ط\": 16,\n",
    "                            \"ظ\": 17,\n",
    "                            \"ع\": 18,\n",
    "                            \"غ\": 19,\n",
    "                            \"ف\": 20,\n",
    "                            \"ق\": 21,\n",
    "                            \"ك\": 22,\n",
    "                            \"ل\": 23,\n",
    "                            \"م\": 24,\n",
    "                            \"ن\": 25,\n",
    "                            \"ه\": 26,\n",
    "                            \"و\": 27,\n",
    "                            \"ي\": 28,\n",
    "                            \"آ\": 29,\n",
    "                            \"إ\": 30,\n",
    "                            \"ئ\": 31,\n",
    "                            \"ء\": 32,\n",
    "                            \"أ\": 33,\n",
    "                            \"ؤ\":34,\n",
    "                            \"ة\":35,\n",
    "                            \"ى\":36,\n",
    "                        }\n",
    "\n",
    "    x_train_letters = []\n",
    "    y_train_letters = []\n",
    "    x_train_words = []\n",
    "    for sentence in dataset_sentences:\n",
    "        sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "        tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "        tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "        sentence_x = list()\n",
    "        sentence_y = list()\n",
    "        for word in tokens:\n",
    "            text, inputs, diacritics =util.extract_haraqat(word)\n",
    "            \n",
    "            word_x = list()\n",
    "            word_y = list()\n",
    "            for i in range(len(inputs)):\n",
    "                # for every char we see its rep in arabic_alphabet and get its 1 hot encoding vector\n",
    "                word_x.append(arabic_alphabet[inputs[i]])\n",
    "                word_y.append(harakat[diacritics[i]])\n",
    "                # word_x.append(inputs[i])\n",
    "                # word_y.append(diacritics[i])\n",
    "            sentence_x.append(word_x)\n",
    "            sentence_y.append(word_y)\n",
    "        if(len(sentence_x) != 0):\n",
    "            # 4d array len of sentences * len of words * len of chars * one hot encoding size\n",
    "            x_train_letters.append(sentence_x)\n",
    "            # 3d array len of sentences * len of words * number of diacritics --> represents the diacritics as indices check (harakat dict) \n",
    "            y_train_letters.append(sentence_y)\n",
    "\n",
    "        if(len(tokens_wihtout_diacratics)) :\n",
    "            x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "\n",
    "    print(y_train_letters[1])\n",
    "    print(len(x_train_letters), len(x_train_letters[1]), len(x_train_letters[1][1]), x_train_letters[1][1][1])\n",
    "    print(x_train_words[1])\n",
    "    return x_train_letters,y_train_letters,x_train_words\n",
    "\n",
    "dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "\n",
    "x_train_chars,y_train_chars,x_train_words = Tokenizing(dataset_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"my_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "\n",
    "word_index_x = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index_x.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "x_train_words_indices = [[word_index_x[word] for word in sentence] for sentence in x_train_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train_words, x_train_chars, y_train_chars, one_hot_enconding):\n",
    "        # the words dataset that will be given to the first model\n",
    "        max_list_length = max(len(_) for _ in x_train_words)\n",
    "        x_padded = [_ + [0] * (max_list_length - len(_)) for _ in x_train_words]\n",
    "        self.tensor_x_words = torch.tensor(x_padded)\n",
    "\n",
    "        x_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in x_train_chars]\n",
    "        #print(x_padded_chars[1])\n",
    "        #print(len(x_padded_chars[1]),len(x_padded_chars[1][1]))\n",
    "\n",
    "        # for sentence in x_train_chars:\n",
    "        #     temp=list()\n",
    "        #     for i in range(max_list_length - len(sentence)):\n",
    "        #         temp.append([])\n",
    "        #     sentence.extend(temp)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in x_train_chars for word in sentence)\n",
    "        for i in range(len(x_padded_chars)): #0->253\n",
    "            for j in range(len(x_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(x_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    x_padded_chars[i][j].extend(temp)\n",
    "        \n",
    "        #print(len(x_padded_chars), len(x_padded_chars[2]), len(x_padded_chars[2][1]), x_padded_chars[2][1][1])\n",
    "        self.tensor_x_chars = torch.tensor(x_padded_chars)\n",
    "\n",
    "        max_char_length = max(len(word) for sentence in y_train_chars for word in sentence)\n",
    "        #for sentence in y_train_chars: \n",
    "            #y_padded_chars.append([word + [0] * (max_char_length - len(word)) for word in sentence])\n",
    "        y_padded_chars = [_ + [[]] * (max_list_length - len(_)) for _ in y_train_chars]    \n",
    "        for i in range(len(y_padded_chars)): #0->253\n",
    "            for j in range(len(y_padded_chars[i])):\n",
    "                temp=list()\n",
    "                for k in range(max_char_length - len(y_padded_chars[i][j])):\n",
    "                    temp.append(0)\n",
    "                #print(temp)\n",
    "                if(len(temp)!=0):\n",
    "                    y_padded_chars[i][j].extend(temp)\n",
    "        self.tensor_y_chars = torch.tensor(y_padded_chars)\n",
    "\n",
    "        #print(len(self.tensor_y_chars),len(self.tensor_y_chars[0]))\n",
    "        #print(self.tensor_x_words[0], self.tensor_x_chars[0], self.tensor_y_chars[0])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.tensor_x_chars)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_chars[idx], self.tensor_y_chars[idx], self.tensor_x_words[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Word2vec Feature Extraction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArabicDataset(x_train_words_indices, x_train_chars, y_train_chars, one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.tensor_x_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> applying Bi-LSTM </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_Lstm(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple,one_hot_enconding:np.array ,batch_size : int=512 ,hot_encoding_vector_len : int=37 , num_chars : int =37 , hidden_size_layer_words : int   =100, hidden_size_layer_chars : int =100, classes : int=15):\n",
    "      super(Bi_Lstm, self).__init__()\n",
    "      self.embedding_words=nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim=embedding_dim[1], _weight=torch.tensor(embedding_matrix))\n",
    "      self.lstm_layer_words = nn.LSTM(embedding_dim[1],hidden_size=hidden_size_layer_words,batch_first=True , bias=True, bidirectional=True)\n",
    "      self.embedding_char=nn.Embedding(num_embeddings = num_chars, embedding_dim=hot_encoding_vector_len,_weight=torch.tensor(one_hot_enconding))\n",
    "      self.lstm_layer_char = nn.LSTM(hidden_size_layer_words,hidden_size=hidden_size_layer_chars,batch_first=True , bias=True, bidirectional=True)\n",
    "      self.linear = nn.Linear(hidden_size_layer_chars, classes, bias=True)\n",
    "      #self.Softmax = nn.Softmax(classes)\n",
    "\n",
    "  def forward(self,sentences,words):\n",
    "      feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "      feature_vector = feature_vector.to(dtype=torch.float)\n",
    "      lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "      print(lstm_output[0].shape)  \n",
    "      # looping over lstm_output to remove unwanted feature vectors\n",
    "      no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "      lstm_output_words= lstm_output[0]\n",
    "      lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "      print(lstm_output_words_2d.shape)\n",
    "      embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "      #words * chars * one hot encoding for each char\n",
    "      embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "      print(embedding_char_output_3d.shape)\n",
    "\n",
    "      print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "      embedding_char_output_3d_concatenated=torch.zeros(embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1])\n",
    "      for word in range(len(lstm_output_words_2d)):\n",
    "            for chars in range(len(embedding_char_output_3d[word])):\n",
    "               embedding_char_output_3d_concatenated[word][chars]  = torch.cat([embedding_char_output_3d[word][chars],lstm_output_words_2d[word]])\n",
    "      final_output=self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "      print(final_output[0].shape)  \n",
    "\n",
    "      #[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل']]\n",
    "      return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lstm_output[1][0:12])\n",
    "# print(x_train_padded.shape)\n",
    "model=Bi_Lstm(embedding_matrix,(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size),one_hot_enconding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 253, 200])\n",
      "torch.Size([63250, 200])\n",
      "torch.Size([63250, 13, 37])\n",
      "tensor([-2.0344e-01,  9.9230e-03,  1.6328e-02, -5.1457e-02, -2.2637e-02,\n",
      "         1.2048e-01,  1.5927e-01,  1.4865e-01,  8.2172e-02,  9.5043e-03,\n",
      "         1.4498e-02, -6.3430e-02,  5.9370e-02, -1.9624e-02,  9.5759e-02,\n",
      "        -1.2517e-01, -4.8731e-02, -2.2047e-04,  7.2794e-02,  1.6152e-02,\n",
      "        -6.2303e-02,  1.3535e-01, -9.4008e-02, -1.3959e-01, -9.5502e-02,\n",
      "         1.1806e-01, -5.3433e-02,  8.5038e-02,  1.1856e-01,  4.5408e-02,\n",
      "        -1.2154e-01, -1.8357e-02, -1.4617e-01,  1.5374e-01,  4.4449e-02,\n",
      "         7.3949e-02,  3.2058e-02,  1.0788e-01, -1.2663e-01, -1.2560e-01,\n",
      "         1.1553e-01, -1.5012e-01,  6.1347e-02,  1.0771e-01, -1.1133e-01,\n",
      "         1.1743e-01,  3.2412e-02,  2.1618e-01, -7.7336e-03, -1.3415e-01,\n",
      "        -1.2283e-01, -2.0505e-01,  2.3538e-02, -3.1232e-02,  1.4656e-01,\n",
      "        -6.1344e-02,  1.3412e-01, -4.8200e-02,  1.9541e-05, -5.9460e-02,\n",
      "         5.2956e-02,  5.6341e-02,  2.6482e-01,  8.1475e-02, -8.4802e-02,\n",
      "        -4.9613e-02,  7.6818e-02, -1.7902e-01, -1.3122e-01,  3.7049e-02,\n",
      "        -4.7059e-02, -9.6772e-02, -1.0888e-01,  8.8377e-02, -7.0648e-02,\n",
      "         1.7656e-01,  2.3978e-01, -7.7088e-02, -1.0087e-02, -2.1174e-02,\n",
      "         8.9027e-03,  9.7461e-02,  1.0571e-01, -3.2771e-02,  1.1333e-01,\n",
      "         4.5333e-02, -1.7693e-01, -8.4085e-02, -9.7248e-02,  6.6323e-02,\n",
      "        -6.5935e-02,  6.2168e-02, -8.1704e-02, -1.1752e-01, -2.5482e-02,\n",
      "        -1.5402e-01, -3.7039e-02,  8.5195e-02,  6.0145e-02, -7.9165e-02,\n",
      "         1.2124e-01,  1.8215e-01,  1.7931e-01, -3.8526e-02, -1.8602e-01,\n",
      "        -7.7602e-02,  7.0161e-02, -1.3764e-01, -1.9492e-01, -1.9950e-01,\n",
      "        -1.4976e-01,  6.4133e-03,  3.4588e-02, -1.5340e-02,  5.9089e-02,\n",
      "         1.1126e-01,  1.4253e-01,  5.6421e-03,  9.1844e-04, -7.3319e-02,\n",
      "         1.6207e-01, -8.2595e-02, -1.4261e-01,  5.8519e-03,  5.7293e-02,\n",
      "        -7.1846e-02, -4.6090e-02,  8.6042e-02, -1.4605e-01, -2.4316e-02,\n",
      "        -6.2030e-02,  3.1681e-01,  1.4141e-01, -1.4599e-02, -4.2484e-02,\n",
      "         1.3360e-01,  3.4186e-02,  1.0737e-01,  1.6956e-01,  1.5971e-01,\n",
      "         1.7919e-01, -1.4202e-01,  1.9193e-01, -9.1397e-02,  2.7978e-02,\n",
      "         2.4511e-02,  4.3030e-02,  7.5651e-02, -1.0159e-01, -7.3067e-02,\n",
      "        -1.0956e-01, -1.0832e-01, -8.2431e-02,  1.7560e-01, -1.2324e-01,\n",
      "        -1.9222e-01,  4.0733e-03, -4.2548e-02,  2.0975e-01,  1.4236e-02,\n",
      "        -1.8477e-01, -2.0393e-02,  5.6887e-02, -6.3147e-02, -2.4068e-01,\n",
      "        -4.9256e-02, -8.7294e-02, -6.5155e-02,  1.0310e-01, -1.2175e-01,\n",
      "         1.3245e-01, -1.2069e-01, -6.0292e-02, -6.1668e-02, -5.1832e-02,\n",
      "         1.4384e-01, -1.5702e-02,  1.4445e-01, -9.5690e-02, -7.5776e-02,\n",
      "         6.7985e-02,  6.3142e-02, -2.4056e-02,  4.1197e-02, -3.4583e-02,\n",
      "        -1.9746e-01, -2.3898e-02,  9.7123e-02,  1.0369e-01, -1.6968e-01,\n",
      "        -1.3349e-01, -2.4235e-02, -1.6017e-02,  1.1451e-01, -1.0491e-01,\n",
      "        -3.4073e-02, -1.9281e-01,  9.9316e-02,  3.7433e-01,  1.1761e-01],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.forward(dataset.tensor_x_words[:250],dataset.tensor_x_chars[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing second model for char level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# looping over lstm_output to remove unwanted feature vectors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m no_of_sentences, length_of_sentence, feature_vector_size \u001b[38;5;241m=\u001b[39m lstm_output\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      4\u001b[0m lstm_output_without_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_of_sentences):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_output' is not defined"
     ]
    }
   ],
   "source": [
    "# looping over lstm_output to remove unwanted feature vectors\n",
    "no_of_sentences, length_of_sentence, feature_vector_size = lstm_output.shape\n",
    "\n",
    "lstm_output_without_padding = list()\n",
    "for i in range(no_of_sentences):\n",
    "    lstm_output_without_padding.extend(lstm_output[i, 1:len(dataset.x_train_words[i])-1, :])\n",
    "\n",
    "# print(lstm_output_without_padding[0:6])\n",
    "\n",
    "\n",
    "# x_train_letter_concatinated = [[np.concatenate(one_hot_enconding[letter_index], lstm_output_without_padding[i]) for letter_index in x_train_letter_indices[i]] for i in range(len(dataset.x_train_letters))]\n",
    "\n",
    "x_train_letter_concatenated = []\n",
    "\n",
    "for i in range(len(dataset.x_train_letters)):\n",
    "    concatenated_arrays = []\n",
    "    for letter in dataset.x_train_letters[i]:\n",
    "        concatenated_arrays.append(np.concatenate([one_hot_enconding[dataset.arabic_alphabet[letter]], lstm_output_without_padding[i]]))\n",
    "    x_train_letter_concatenated.extend(concatenated_arrays)\n",
    "\n",
    "\n",
    "#[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letter = [dataset.harakat[diacritic] for diacritic in dataset.y_train_letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model_char = Sequential()\n",
    "first_bidirectional_layer = Bidirectional(LSTM(units=64, return_sequences=True))\n",
    "bi_lstm_model_char.add(Dense(255, activation='softmax'))\n",
    "\n",
    "bi_lstm_model_char.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bi_lstm_model_char.fit(np.array(x_train_letter_concatenated), np.array(y_train_letter), epochs=10, batch_size=32)\n",
    "score = bi_lstm_model_char.evaluate(np.array(x_train_letter_concatenated), np.array(y_train_letter))\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "bi_lstm_model_char.save(\"char-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(max_word_length,))\n",
    "word_embedding = Embedding(input_dim=num_words, output_dim=embedding_dim)(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
